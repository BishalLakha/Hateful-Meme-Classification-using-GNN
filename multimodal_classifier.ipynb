{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "import numpy as np\n",
    "\n",
    "from utils.utils import *\n",
    "from models.gcn import GCN\n",
    "from models.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/Projects/Classes/Deeplearning_class/project/utils/utils.py:242: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    }
   ],
   "source": [
    "# Loading graph\n",
    "adj, features,base_features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(word2vec=True)\n",
    "\n",
    "features_with_word2vec = preprocess_features(features)\n",
    "\n",
    "features = sp.identity(features.shape[0])\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/Projects/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Loading image features\n",
    "train_embeddings, test_embeddings = get_image_embeddings()\n",
    "training_embeddings = torch.tensor(train_embeddings).reshape(train_size,512)\n",
    "test_embeddings = torch.tensor(test_embeddings).reshape(test_size,512)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Wac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = \"./data/additional_data/meme_vocab.txt\"\n",
    "wac_data = loadWAC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = open(vocab_file, 'r')\n",
    "all_word_embeddings = []\n",
    "for word in vocab.readlines():\n",
    "    word = word.strip()\n",
    "\n",
    "    try:\n",
    "        word_embedding = wac_data[word]\n",
    "        word_embedding = torch.tensor(word_embedding).reshape(1,512)\n",
    "    except:\n",
    "        word_embedding = torch.zeros((1,512))\n",
    "    all_word_embeddings.append(word_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings_words  = torch.cat(all_word_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine wac and image representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting complete features for all nodes\n",
    "word_nodes = features.shape[0] - train_size - test_size\n",
    "\n",
    "# Since we don't have image embeddings for words, we will use zeros\n",
    "\n",
    "all_image_features = torch.cat((training_embeddings, image_embeddings_words, test_embeddings), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0091, -0.0294,  0.1800,  ...,  0.1068,  0.0904,  0.0022],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2614, -0.0073, -0.1738,  ...,  0.4274,  0.2315,  0.0266],\n",
       "        ...,\n",
       "        [-0.0298,  0.0187, -0.1962,  ..., -0.1138,  0.0497,  0.0509],\n",
       "        [ 0.0569,  0.1939, -0.0819,  ...,  0.2075, -0.2659, -0.0912],\n",
       "        [-0.2407,  0.0551, -0.0462,  ...,  0.1517,  0.2010,  0.0911]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj + sp.eye(adj.shape[0]))\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "adjdense = torch.from_numpy(pre_adj(adj).A.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "def construct_graph(adjacency):\n",
    "    g = DGLGraph()\n",
    "    adj = pre_adj(adjacency)\n",
    "    g.add_nodes(adj.shape[0])\n",
    "    g.add_edges(adj.row,adj.col)\n",
    "    adjdense = adj.A\n",
    "    adjd = np.ones((adj.shape[0]))\n",
    "    for i in range(adj.shape[0]):\n",
    "        adjd[i] = adjd[i] * np.sum(adjdense[i,:])\n",
    "    weight = torch.from_numpy(adj.data.astype(np.float32))\n",
    "    g.ndata['d'] = torch.from_numpy(adjd.astype(np.float32))\n",
    "    g.edata['w'] = weight\n",
    "\n",
    "    if CUDA:\n",
    "        g = g.to(torch.device('cuda:0'))\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self,g,in_feats,out_feats,activation,feat_drop=True):\n",
    "        super(SimpleConv, self).__init__()\n",
    "        self.graph = g\n",
    "        self.activation = activation\n",
    "        #self.reset_parameters()\n",
    "        setattr(self, 'W', nn.Parameter(torch.randn(in_feats,out_feats)))\n",
    "        #self.b = nn.Parameter(torch.zeros(1, out_feats))\n",
    "        #self.linear = nn.Linear(in_feats,out_feats)\n",
    "        self.feat_drop = feat_drop\n",
    "    \n",
    "    # def reset_parameters(self):\n",
    "    #     gain = nn.init.calculate_gain('relu')\n",
    "    #     nn.init.xavier_uniform_(self.linear.weight,gain=gain)\n",
    "    \n",
    "    def forward(self, feat):\n",
    "        g = self.graph.local_var()\n",
    "        g.ndata['h'] = feat.mm(getattr(self, 'W'))\n",
    "        g.update_all(fn.src_mul_edge(src='h', edge='w', out='m'), fn.sum(msg='m',out='h'))\n",
    "        rst = g.ndata['h']\n",
    "        #rst = self.linear(rst)\n",
    "        rst = self.activation(rst)\n",
    "        return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, g, in_feats, out_feats):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.graph = g\n",
    "        setattr(self, 'W', nn.Parameter(torch.randn(in_feats,out_feats)))\n",
    "        setattr(self, 'al', nn.Parameter(torch.randn(in_feats,1)))\n",
    "        setattr(self, 'ar', nn.Parameter(torch.randn(in_feats,1)))\n",
    "\n",
    "    def forward(self, feat):\n",
    "        # equation (1)\n",
    "        g = self.graph.local_var()\n",
    "        g.ndata['h'] = feat.mm(getattr(self, 'W'))\n",
    "        g.ndata['el'] = feat.mm(getattr(self, 'al'))\n",
    "        g.ndata['er'] = feat.mm(getattr(self, 'ar'))\n",
    "        g.apply_edges(fn.u_add_v('el', 'er', 'e'))\n",
    "        # message passing\n",
    "        g.update_all(fn.src_mul_edge('h', 'w', 'm'), fn.sum('m', 'h'))\n",
    "        e = F.leaky_relu(g.edata['e'])\n",
    "        # compute softmax\n",
    "        g.edata['w'] = F.softmax(e)\n",
    "        rst = g.ndata['h']\n",
    "        #rst = self.linear(rst)\n",
    "        #rst = self.activation(rst)\n",
    "        return rst\n",
    "\n",
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim, activation, num_heads=2, merge=None):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, h):\n",
    "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # concat on the output feature dimension (dim=1)\n",
    "            x = torch.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            # merge using average\n",
    "            x = torch.mean(torch.stack(head_outs),dim=0)\n",
    "        \n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEMeanConv(nn.Module):\n",
    "    def __init__(self,g,in_feats,out_feats,activation):\n",
    "        super(SAGEMeanConv, self).__init__()\n",
    "        self.graph = g\n",
    "        self.feat_drop = nn.Dropout(0.5)\n",
    "        setattr(self, 'W', nn.Parameter(torch.randn(in_feats,out_feats)))\n",
    "        #self.linear = nn.Linear(in_feats, out_feats, bias=True)\n",
    "        setattr(self, 'Wn', nn.Parameter(torch.randn(out_feats,out_feats)))\n",
    "        self.activation = activation\n",
    "        #self.neigh_linear = nn.Linear(out_feats, out_feats, bias=True)\n",
    "        # self.reset_parameters()\n",
    "    \n",
    "    '''\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_uniform_(self.linear.weight,gain=gain)\n",
    "        nn.init.xavier_uniform_(self.neigh_linear.weight,gain=gain)\n",
    "    '''\n",
    "    \n",
    "    def forward(self,feat):\n",
    "        g = self.graph.local_var()\n",
    "        #feat = self.feat_drop(feat)\n",
    "        h_self = feat.mm(getattr(self, 'W'))\n",
    "        g.ndata['h'] = h_self\n",
    "        g.update_all(fn.copy_src('h', 'm'), fn.sum('m', 'neigh'))\n",
    "        h_neigh = g.ndata['neigh']\n",
    "        degs = g.in_degrees().float()\n",
    "        degs = degs.to(torch.device('cuda:0'))\n",
    "        g.ndata['h'] = (h_neigh + h_self) / (degs.unsqueeze(-1) + 1)\n",
    "        rst = g.ndata['h']\n",
    "        rst = self.activation(rst)\n",
    "        # rst = th.norm(rst)\n",
    "        return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_word2vec = torch.tensor(features_with_word2vec).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifer(nn.Module):\n",
    "    def __init__(self,g,input_dim,num_classes,conv):\n",
    "        super(Classifer, self).__init__()\n",
    "        self.data_graph = g\n",
    "        self.GCN = conv\n",
    "        self.gcn1 = self.GCN(g,input_dim, 300, F.relu)\n",
    "        self.gcn2 = self.GCN(g, 300, 200, F.relu)\n",
    "        self.gcn3 = self.GCN(g, 200, num_classes, F.relu)\n",
    "\n",
    "    \n",
    "    def forward(self, features):\n",
    "        x = self.gcn1(features)\n",
    "\n",
    "        # To Do: Fuse the text embedding with image embedding \n",
    "        self.embedding = x\n",
    "        # x = torch.cat(x,g.ndata['x'])\n",
    "        # x = torch.cat((self.embedding,g.ndata['x']),dim=1)\n",
    "        x = self.gcn2(x)\n",
    "        x = self.gcn3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ClassiferFusion(nn.Module):\n",
    "    def __init__(self,g,input_dim,num_classes,conv):\n",
    "        super(ClassiferFusion, self).__init__()\n",
    "        self.data_graph = g\n",
    "        self.GCN = conv\n",
    "        self.gcn1 = self.GCN(g,input_dim, 300, F.relu)\n",
    "        self.gcn2 = self.GCN(g, 812, 416, F.relu)\n",
    "        self.gcn3 = self.GCN(g, 416,num_classes , F.relu)\n",
    "        # self.gcn4 = self.GCN(g, 300, num_classes, F.relu)\n",
    "        # self.gcn5 = self.GCN(g, 300, num_classes, F.relu)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, features):\n",
    "        x = self.gcn1(features)\n",
    "\n",
    "        self.embedding1 = x\n",
    "        # x = torch.cat(x,g.ndata['x'])\n",
    "        # Mean of text embedding and word2vec\n",
    "        x = torch.mean(torch.stack((self.embedding1,features_with_word2vec.float())),dim=0)\n",
    "\n",
    "        # # Concatenate the text embedding with image embedding \n",
    "        x = torch.cat((x,g.ndata['image_embeddings']),dim=1)\n",
    "       \n",
    "        x = self.gcn2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.gcn3(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.gcn4(x)\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.gcn5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/Projects/venv3.6/lib/python3.6/site-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "g = construct_graph(adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding node features image_embeddings\n",
    "all_image_features = all_image_features.cuda()\n",
    "g.ndata['image_embeddings'] = all_image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = ClassiferFusion(g,input_dim=features.shape[0], num_classes=y_train.shape[1],conv=MultiHeadGATLayer)\n",
    "# model = Classifer(g,input_dim=features.shape[0], num_classes=y_train.shape[1],conv=MultiHeadGATLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "t_features = torch.from_numpy(features.astype(np.float32))\n",
    "t_y_train = torch.from_numpy(y_train)\n",
    "t_y_val = torch.from_numpy(y_val)\n",
    "t_y_test = torch.from_numpy(y_test)\n",
    "t_train_mask = torch.from_numpy(train_mask.astype(np.float32))\n",
    "tm_train_mask = torch.transpose(torch.unsqueeze(t_train_mask, 0), 1, 0).repeat(1, y_train.shape[1])\n",
    "support = [preprocess_adj(adj)]\n",
    "num_supports = 1\n",
    "t_support = []\n",
    "for i in range(len(support)):\n",
    "    t_support.append(torch.Tensor(support[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_features = t_features.cuda()\n",
    "t_y_train = t_y_train.cuda()\n",
    "#t_y_val = t_y_val.cuda()\n",
    "#t_y_test = t_y_test.cuda()\n",
    "t_train_mask = t_train_mask.cuda()\n",
    "tm_train_mask = tm_train_mask.cuda()\n",
    "# for i in range(len(support)):\n",
    "#     t_support = [t.cuda() for t in t_support if True]\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(features, labels, mask):\n",
    "    t_test = time.time()\n",
    "    # feed_dict_val = construct_feed_dict(\n",
    "    #     features, support, labels, mask, placeholders)\n",
    "    # outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels], feed_dict=feed_dict_val)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features).cpu()\n",
    "        t_mask = torch.from_numpy(np.array(mask*1., dtype=np.float32))\n",
    "        tm_mask = torch.transpose(torch.unsqueeze(t_mask, 0), 1, 0).repeat(1, labels.shape[1])\n",
    "        loss = criterion(logits * tm_mask, torch.max(labels, 1)[1])\n",
    "        pred = torch.max(logits, 1)[1]\n",
    "        acc = ((pred == torch.max(labels, 1)[1]).float() * t_mask).sum().item() / t_mask.sum().item()\n",
    "        \n",
    "    return loss.numpy(), acc, pred.numpy(), labels.numpy(), (time.time() - t_test)\n",
    "\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/Projects/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ClassiferFusion' object has no attribute 'embedding2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c4a3f37dd829>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtm_train_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt_train_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mt_train_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/venv3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-2aa98b5b6924>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# # Concatenate the text embedding with image embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/venv3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ClassiferFusion' object has no attribute 'embedding2'"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(t_features)\n",
    "    loss = criterion(logits * tm_train_mask, torch.max(t_y_train, 1)[1])    \n",
    "    acc = ((torch.max(logits, 1)[1] == torch.max(t_y_train, 1)[1]).float() * t_train_mask).sum().item() / t_train_mask.sum().item()\n",
    "        \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_acc, pred, labels, duration = evaluate(t_features, t_y_val, val_mask)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print_log(\"Epoch: {:.0f}, train_loss= {:.5f}, train_acc= {:.5f}, val_loss= {:.5f}, val_acc= {:.5f}, time= {:.5f}\"\\\n",
    "                .format(epoch + 1, loss, acc, val_loss, val_acc, time.time() - t))\n",
    "\n",
    "    # if epoch > 5 and val_losses[-1] > np.mean(val_losses[-(5+1):-1]):\n",
    "    #     print_log(\"Early stopping...\")\n",
    "    #     break\n",
    "\n",
    "\n",
    "print_log(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/Projects/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022/4/20 13:44:45] Test set results: \n",
      "[2022/4/20 13:44:45] \t loss= 0.70657, accuracy= 0.51600, time= 0.48989\n",
      "[2022/4/20 13:44:45] Test Precision, Recall and F1-Score...\n",
      "[2022/4/20 13:44:45]               precision    recall  f1-score   support\n",
      "[2022/4/20 13:44:45] \n",
      "[2022/4/20 13:44:45]            0     0.5159    0.8294    0.6361       510\n",
      "[2022/4/20 13:44:45]            1     0.5167    0.1898    0.2776       490\n",
      "[2022/4/20 13:44:45] \n",
      "[2022/4/20 13:44:45]     accuracy                         0.5160      1000\n",
      "[2022/4/20 13:44:45]    macro avg     0.5163    0.5096    0.4569      1000\n",
      "[2022/4/20 13:44:45] weighted avg     0.5163    0.5160    0.4604      1000\n",
      "[2022/4/20 13:44:45] \n",
      "[2022/4/20 13:44:45] Macro average Test Precision, Recall and F1-Score...\n",
      "[2022/4/20 13:44:45] (0.5162601626016261, 0.5096038415366146, 0.4568510829312087, None)\n",
      "[2022/4/20 13:44:45] Micro average Test Precision, Recall and F1-Score...\n",
      "[2022/4/20 13:44:45] (0.516, 0.516, 0.516, None)\n",
      "[2022/4/20 13:44:45] Auc Score test ...\n",
      "[2022/4/20 13:44:45] 0.5096038415366146\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_loss, test_acc, pred, labels, test_duration = evaluate(t_features, t_y_test, test_mask)\n",
    "print_log(\"Test set results: \\n\\t loss= {:.5f}, accuracy= {:.5f}, time= {:.5f}\".format(test_loss, test_acc, test_duration))\n",
    "\n",
    "test_pred = []\n",
    "test_labels = []\n",
    "for i in range(len(test_mask)):\n",
    "    if test_mask[i]:\n",
    "        test_pred.append(pred[i])\n",
    "        test_labels.append(np.argmax(labels[i]))\n",
    "\n",
    "\n",
    "print_log(\"Test Precision, Recall and F1-Score...\")\n",
    "print_log(metrics.classification_report(test_labels, test_pred, digits=4))\n",
    "print_log(\"Macro average Test Precision, Recall and F1-Score...\")\n",
    "print_log(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\n",
    "print_log(\"Micro average Test Precision, Recall and F1-Score...\")\n",
    "print_log(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n",
    "\n",
    "print_log(\"Auc Score test ...\")\n",
    "print_log(metrics.roc_auc_score(test_labels, test_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9188de91555ba3328945728ae07a43dc8dfe643542d510c5729b810ec16c80f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 ('venv3.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
